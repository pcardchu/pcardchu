{
  "metadata": {
    "name": "Top3Category",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nspark \u003d SparkSession.builder.master(\"yarn\").appName(\"card 201909 test1\").config(\"spark.cassandra.connection.host\", \"172.21.0.6\").getOrCreate()\n\nfile_path \u003d \"hdfs:///example/card_201909.csv\"\nfile_path2 \u003d \"hdfs:///example/card_201910.csv\"\nfile_path3 \u003d \"hdfs:///example/card_201911.csv\"\nfile_path4 \u003d \"hdfs:///example/card_201912.csv\"\n\ndf1 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path)\ndf2 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path2)\ndf3 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path3)\ndf4 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path4)\n\ndf \u003d df1.union(df2).union(df3).union(df4)\ndf.show()\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.types import IntegerType\n\ndfselect \u003d df.select(\n        F.substring(df.성별, 3,2).alias(\"성별\"),\n        F.substring(df.연령, 3, 3).alias(\"연령대\"),\n        df.개인기업구분,\n        df.매출년월일,\n        df.승인시간대1.cast(IntegerType()),\n        df.결제금액.cast(IntegerType()),\n        df.가맹점업종코드\n    )\n\n\ndfselect.show()\n# dfselect.createOrReplaceTempView(\"card\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import dense_rank, col, count\nfrom pyspark.sql.window import Window\n\n# 성별과 연령대별로 가맹점 업종코드의 사용 빈도수를 계산\nagg_df \u003d dfselect.filter((col(\"성별\") !\u003d \"기업\") \u0026 (col(\"연령대\") !\u003d \"기타\")).groupBy(\"성별\", \"연령대\", \"가맹점업종코드\").count()\n\n# Window 함수를 정의하여 각 그룹별로 가장 빈번한 업종코드 상위 3개를 추출할 수 있도록 설정\nwindowSpec \u003d Window.partitionBy(\"성별\", \"연령대\").orderBy(col(\"count\").desc())\n\n# dense_rank를 사용하여 각 그룹 내에서 빈도수에 따른 순위를 매김\nranked_df \u003d agg_df.withColumn(\"rank\", dense_rank().over(windowSpec))\n\n# 빈도수가 높은 상위 3개의 업종코드만 필터링\ntop3_df \u003d ranked_df.filter(col(\"rank\") \u003c\u003d 3)\n\n# 결과 출력\ntop3_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import monotonically_increasing_id\n\ntop3category_df \u003d top3_df.select(\n        monotonically_increasing_id().alias(\u0027id\u0027),\n        top3_df.성별.alias(\u0027gender\u0027),\n        top3_df.연령대.alias(\"age\"),\n        top3_df.가맹점업종코드.alias(\u0027category\u0027)\n        )\n# df_test \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"top3category\", keyspace\u003d\"trend\").load()\n  \ntop3category_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntop3category_df.write.format(\u0027org.apache.spark.sql.cassandra\u0027).mode(\u0027append\u0027).option(\"keyspace\", \"trend\").option(\"table\", \"top3category\").option(\"spark.cassandra.output.consistency.level\", \"ONE\").save()\n\n# 테스트\ndf_test \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"top3category\", keyspace\u003d\"trend\").load()\ndf_test.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}