{
  "metadata": {
    "name": "PeakTime",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nspark \u003d SparkSession.builder.master(\"yarn\").appName(\"card 201909 test1\").config(\"spark.cassandra.connection.host\", \"172.21.0.6\").getOrCreate()\n\nfile_path \u003d \"hdfs:///example/card_201909.csv\"\nfile_path2 \u003d \"hdfs:///example/card_201910.csv\"\nfile_path3 \u003d \"hdfs:///example/card_201911.csv\"\nfile_path4 \u003d \"hdfs:///example/card_201912.csv\"\n\ndf1 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path)\ndf2 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path2)\ndf3 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path3)\ndf4 \u003d spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").option(\"encoding\", \"EUC-KR\").load(file_path4)\n\ndf \u003d df1.union(df2).union(df3).union(df4)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.types import IntegerType\n\ndfselect \u003d df.select(\n        F.substring(df.성별, 3,2).alias(\"성별\"),\n        F.substring(df.연령, 3, 3).alias(\"연령대\"),\n        df.개인기업구분,\n        df.매출년월일,\n        df.승인시간대1.cast(IntegerType()),\n        df.결제금액.cast(IntegerType()),\n        df.가맹점업종코드\n    )\n\n\ndfselect.show()\n# dfselect.createOrReplaceTempView(\"card\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import dense_rank, col, count\nfrom pyspark.sql.window import Window\n\n# 승인시간대1별로 그룹화하고, 각 연령대의 빈도수 계산\nageGroupCount \u003d dfselect.filter((col(\"성별\") !\u003d \"기업\") \u0026 (col(\"연령대\") !\u003d \"기타\")).groupBy(\"승인시간대1\", \"연령대\").count()\n\n# ageGroupCount.show()\n# 전체 레코드 수에 대한 각 연령대별 count의 비율 계산\ntotalCounts \u003d df.groupBy(\"승인시간대1\").count().withColumnRenamed(\"count\",\"total_counts\")\nageGroupWithTotal \u003d ageGroupCount.join(totalCounts, \"승인시간대1\")\nageGroupWithRatio \u003d ageGroupWithTotal.withColumn(\"ratio\", col(\"count\") / col(\"total_counts\"))\n\n# 각 승인시간대별로 가장 높은 비율을 가진 연령대 선택\nwindowSpec \u003d Window.partitionBy(\"승인시간대1\").orderBy(col(\"ratio\").desc())\n\nmostRepresentativeAgeGroup \u003d ageGroupWithRatio.withColumn(\"rank\", dense_rank().over(windowSpec))\\\n                                               .filter(col(\"연령대\") \u003d\u003d \u002720대\u0027)\\\n                                               .select(\"승인시간대1\", \"연령대\", \"ratio\")\n\nmostRepresentativeAgeGroup.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.sql.functions import dense_rank, col, count\r\nfrom pyspark.sql.window import Window\r\n\r\ncalcdf \u003d dfselect.filter((col(\"성별\") !\u003d \"기업\") \u0026 (col(\"연령대\") !\u003d \"기타\"))\r\n\r\n# 전체 연령대별 사용자 수 계산\r\ntotalUsageByAgeGroup \u003d calcdf.groupBy(\"연령대\").count().withColumnRenamed(\"count\", \"total_count\")\r\n\r\n# 각 시간대와 연령대별 사용자 수 계산\r\nusageByTimeAndAge \u003d calcdf.groupBy(\"승인시간대1\", \"연령대\").count()\r\n\r\n# 전체 연령대별 사용자 수를 기준으로 각 시간대별 사용자 수의 비율 계산\r\nusageRatio \u003d usageByTimeAndAge.join(totalUsageByAgeGroup, \"연령대\")\\\r\n                              .withColumn(\"비율\", col(\"count\") / col(\"total_count\"))\\\r\n                              .select(\"승인시간대1\", \"연령대\", \"비율\")\r\n\r\n# 각 시간대별로 가장 높은 비율을 가진 연령대 찾기\r\nwindowSpec \u003d Window.partitionBy(\"승인시간대1\").orderBy(col(\"비율\").desc())\r\n\r\nmostRepresentativeAgeGroup \u003d usageRatio.withColumn(\"rank\", dense_rank().over(windowSpec))\\\r\n                                       .filter(col(\"rank\") \u003d\u003d 1)\\\r\n                                       .select(\"승인시간대1\", \"연령대\", \"비율\")\r\n\r\nmostRepresentativeAgeGroup.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npeaktime_df \u003d mostRepresentativeAgeGroup.select(\n        mostRepresentativeAgeGroup.승인시간대1.alias(\u0027time\u0027),\n        mostRepresentativeAgeGroup.연령대.alias(\"age\")\n        )\n# df_test \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"top3category\", keyspace\u003d\"trend\").load()\n  \npeaktime_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npeaktime_df.write.format(\u0027org.apache.spark.sql.cassandra\u0027).mode(\u0027append\u0027).option(\"keyspace\", \"trend\").option(\"table\", \"peaktime\").option(\"spark.cassandra.output.consistency.level\", \"ONE\").save()\n\n# 테스트\ndf_test \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"peaktime\", keyspace\u003d\"trend\").load()\ndf_test.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}