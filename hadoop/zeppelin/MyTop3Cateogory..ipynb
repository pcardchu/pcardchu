{
  "metadata": {
    "name": "MyTop3Cateogory",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nspark \u003d SparkSession.builder.master(\"yarn\").appName(\"card 201909 test1\").config(\"spark.cassandra.connection.host\", \"172.21.0.6\").getOrCreate()\n\ndf \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"cardhistory\", keyspace\u003d\"trend\").load()\n\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import rank, col, count, when\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.window import Window\n\ntmp_df \u003d df.select(df.userid, F.substring(df.date, 1,6).alias(\u0027date\u0027), df.category, df.amount)\ntmp_df \u003d tmp_df.withColumn(\"amount\", col(\"amount\").cast(IntegerType()))\n\n\n# userid, date, category별로 amount 합계 계산\ndf_grouped \u003d tmp_df.groupBy(\"userid\", \"date\", \"category\").agg(F.sum(\"amount\").alias(\"total_amount\"))\n\n# 각 userid, date 조합 내에서 total_amount에 따른 순위를 매김\nwindowSpec \u003d Window.partitionBy(\"userid\", \"date\").orderBy(col(\"total_amount\").desc())\ndf_ranked \u003d df_grouped.withColumn(\"rank\", F.row_number().over(windowSpec))\n\n# 상위 4개 category 선택, 나머지는 \u0027etc\u0027로 묶기\ndf_final \u003d df_ranked.withColumn(\"category\",\n                                 when(col(\"rank\") \u003c\u003d 4, col(\"category\")).otherwise(\"etc\"))\n\n# 다시 그룹화하여 etc를 포함한 새로운 합계 계산\ndf_result \u003d df_final.groupBy(\"userid\", \"date\", \"category\").agg(F.sum(\"total_amount\").alias(\"total_amount\"))\n\n# 결과 출력\ndf_result.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import monotonically_increasing_id\n\nmy_consumption \u003d df_result.select(monotonically_increasing_id().alias(\u0027id\u0027),\ndf_result.userid,\ndf_result.date,\ndf_result.category,\ndf_result.total_amount.alias(\"totalamount\"))\n\n# 결과 출력\nmy_consumption.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmy_consumption.write.format(\u0027org.apache.spark.sql.cassandra\u0027).mode(\u0027append\u0027).option(\"keyspace\", \"trend\").option(\"table\", \"myconsumption\").option(\"spark.cassandra.output.consistency.level\", \"ONE\").save()\n\n# 테스트\ndf_test \u003d spark.read.format(\"org.apache.spark.sql.cassandra\").options(table\u003d\"myconsumption\", keyspace\u003d\"trend\").load()\ndf_test.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}